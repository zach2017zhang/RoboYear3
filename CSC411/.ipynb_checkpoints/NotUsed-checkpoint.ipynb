{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbability(trainingProbability,realProbability, fakeProbability, wordBase, m, pHat):\n",
    "    words = []\n",
    "    realPs = []\n",
    "    fakePs = []\n",
    "    \n",
    "    for testWord in wordBase:\n",
    "        logProb = 0   \n",
    "        for word in trainingProbability['real'].keys():\n",
    "            if word == testWord:\n",
    "                logProb += log(trainingProbability['real'][word])\n",
    "            else:\n",
    "                logProb += log(1-trainingProbability['real'][word])\n",
    "        logProb += log(realProbability)\n",
    "        realProb = logProb\n",
    "        \n",
    "        logProb = 0   \n",
    "        for word in trainingProbability['fake'].keys():\n",
    "            if word == testWord:\n",
    "                logProb += log(trainingProbability['fake'][word])\n",
    "            else:\n",
    "                logProb += log(1-trainingProbability['fake'][word])\n",
    "        logProb += log(fakeProbability)\n",
    "        fakeProb = logProb\n",
    "        \n",
    "        words.append(testWord)\n",
    "        realPs.append(realProb)\n",
    "        fakePs.append(fakeProb)\n",
    "        \n",
    "    return words, realPs, fakePs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3aNaiveVersion(seed = 0):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "    validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "    validationCount, testCount = count(trainingSet, validationSet, testSet)\n",
    "\n",
    "    m = 0.1\n",
    "    pHat = 0.0001\n",
    "    trainingProbability, realProbability, fakeProbability = probability(trainingRealCount,trainingFakeCount, trainingCount, m, pHat)\n",
    "    realWord = trainingProbability['real'].keys()\n",
    "    fakeWord = trainingProbability['fake'].keys()\n",
    "    realProbability = [trainingProbability['real'][prob] for prob in realWord]\n",
    "    fakeProbability = [trainingProbability['fake'][prob] for prob in fakeWord]\n",
    "    \n",
    "    realResult = []\n",
    "    for i in range(10):\n",
    "        realResult.append(realWord.pop(realProbability.index(max(realProbability))))\n",
    "        realProbability.remove(max(realProbability))\n",
    "    \n",
    "    fakeResult = []\n",
    "    for i in range(10):\n",
    "        fakeResult.append(fakeWord.pop(fakeProbability.index(max(fakeProbability))))\n",
    "        fakeProbability.remove(max(fakeProbability))\n",
    "        \n",
    "    print realResult\n",
    "    print fakeResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3a1(seed = 0):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "    validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "    validationCount, testCount = count(trainingSet, validationSet, testSet)\n",
    "\n",
    "    m = 0.1\n",
    "    pHat = 0.0001\n",
    "    trainingProbability, realProbability, fakeProbability = \\\n",
    "    probability(trainingRealCount,trainingFakeCount, trainingCount, m,pHat)\n",
    "\n",
    "    wordBase = generateWordBase(trainingSet)\n",
    "    wordsOriginal, realPs, fakePs = getProbability(trainingProbability,realProbability, fakeProbability, wordBase, m, pHat)\n",
    "    \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    realResultMax = []\n",
    "    for i in range(10):\n",
    "        realResultMax.append(words.pop(realPs.index(max(realPs))))\n",
    "        realPs.remove(max(realPs))\n",
    "\n",
    "    words = deepcopy(wordsOriginal)\n",
    "    realResultMin = []\n",
    "    for i in range(10):\n",
    "        realResultMin.append(words.pop(realPs.index(min(realPs))))\n",
    "        realPs.remove(min(realPs))\n",
    "    \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    fakeResultMax = []\n",
    "    for i in range(10):\n",
    "        fakeResultMax.append(words.pop(fakePs.index(max(fakePs))))\n",
    "        fakePs.remove(max(fakePs))\n",
    "        \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    fakeResultMin = []\n",
    "    for i in range(10):\n",
    "        fakeResultMin.append(words.pop(fakePs.index(min(fakePs))))\n",
    "        fakePs.remove(min(fakePs))\n",
    "    \n",
    "    \n",
    "    print 'Words whose presence strongly predicts that the news is real:', realResultMax\n",
    "    print 'Words whose absence strongly predicts that the news is real:', realResultMin\n",
    "    print 'Words whose presence strongly predicts that the news is fake:', fakeResultMax\n",
    "    print 'Words whose absence strongly predicts that the news is fake:', fakeResultMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3bNaiveVersion(seed = 0):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "    validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "    validationCount, testCount = count(trainingSet, validationSet, testSet)\n",
    "\n",
    "    m = 0.1\n",
    "    pHat = 0.0001\n",
    "    trainingProbability, realProbability, fakeProbability = probability(trainingRealCount,trainingFakeCount, trainingCount, m, pHat)\n",
    "    realWord = trainingProbability['real'].keys()\n",
    "    fakeWord = trainingProbability['fake'].keys()\n",
    "    realProbability = [trainingProbability['real'][prob] for prob in realWord]\n",
    "    fakeProbability = [trainingProbability['fake'][prob] for prob in fakeWord]\n",
    "    \n",
    "    realResult = []\n",
    "    for i in range(10):\n",
    "        realResult.append(realWord.pop(realProbability.index(max(realProbability))))\n",
    "        realProbability.remove(max(realProbability))\n",
    "    \n",
    "    fakeResult = []\n",
    "    for i in range(10):\n",
    "        fakeResult.append(fakeWord.pop(fakeProbability.index(max(fakeProbability))))\n",
    "        fakeProbability.remove(max(fakeProbability))\n",
    "        \n",
    "    print realResult\n",
    "    print fakeResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3b1(seed = 0):\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "    validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "    validationCount, testCount = count(trainingSet, validationSet, testSet)\n",
    "\n",
    "    m = 0.1\n",
    "    pHat = 0.0001\n",
    "    trainingProbability, realProbability, fakeProbability = \\\n",
    "    probability(trainingRealCount,trainingFakeCount, trainingCount, m,pHat)\n",
    "\n",
    "    wordBase = generateWordBase(trainingSet)\n",
    "    \n",
    "    for word in ENGLISH_STOP_WORDS:\n",
    "        if word in wordBase:\n",
    "            wordBase.remove(word)\n",
    "    \n",
    "    wordsOriginal, realPs, fakePs = getProbability(trainingProbability,realProbability, fakeProbability, wordBase, m, pHat)\n",
    "    \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    realResultMax = []\n",
    "    for i in range(10):\n",
    "        realResultMax.append(words.pop(realPs.index(max(realPs))))\n",
    "        realPs.remove(max(realPs))\n",
    "\n",
    "    words = deepcopy(wordsOriginal)\n",
    "    fakeResultMax = []\n",
    "    for i in range(10):\n",
    "        fakeResultMax.append(words.pop(fakePs.index(max(fakePs))))\n",
    "        fakePs.remove(max(fakePs))\n",
    "    \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    realResultMin = []\n",
    "    for i in range(10):\n",
    "        realResultMin.append(words.pop(realPs.index(min(realPs))))\n",
    "        realPs.remove(min(realPs))\n",
    "        \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    fakeResultMin = []\n",
    "    for i in range(10):\n",
    "        fakeResultMin.append(words.pop(fakePs.index(min(fakePs))))\n",
    "        fakePs.remove(min(fakePs))\n",
    "        \n",
    "    print 'Words whose presence strongly predicts that the news is real:', realResultMax\n",
    "    print 'Words whose absence strongly predicts that the news is real:', realResultMin\n",
    "    print 'Words whose presence strongly predicts that the news is fake:', fakeResultMax\n",
    "    print 'Words whose absence strongly predicts that the news is fake:', fakeResultMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
