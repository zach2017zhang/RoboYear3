{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC411 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import urllib\n",
    "from copy import deepcopy\n",
    "# Constants\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used in multiple parts\n",
    "# -----------------------------------------------------------------------------\n",
    "def getNews(seed = 0):\n",
    "    \n",
    "    fakeFile = open(\"clean_fake.txt\",'r')\n",
    "    fakeNews = fakeFile.read().splitlines()\n",
    "    fakeFile.close()\n",
    "    realFile = open(\"clean_real.txt\",'r')\n",
    "    realNews = realFile.read().splitlines()\n",
    "    realFile.close()\n",
    "    \n",
    "    np.random.shuffle(fakeNews)\n",
    "    np.random.shuffle(realNews)\n",
    "    \n",
    "    return fakeNews, realNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSetSplit(fakeNews, realNews,seed = 0):\n",
    "    \n",
    "    trainingSet ={'real': realNews[:int(0.7*len(realNews))], 'fake':fakeNews[:int(0.7*len(fakeNews))]} \n",
    "    validationSet = {'real':realNews[int(0.7*len(realNews)):int(0.85*len(realNews))], 'fake':fakeNews[int(0.7*len(fakeNews)):int(0.85*len(fakeNews))]}\n",
    "    testSet = {'real': realNews[int(0.85*len(realNews)):] , 'fake': fakeNews[int(0.85*len(fakeNews)):]}\n",
    "\n",
    "    return trainingSet, validationSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(trainingSet, validationSet, testSet):\n",
    "    trainingCount = {'real':{}, 'fake':{}}\n",
    "    validationCount = {'real':{}, 'fake':{}}\n",
    "    testCount = {'real':{}, 'fake':{}}\n",
    "    \n",
    "    for rof in ['real','fake']:\n",
    "        for news in trainingSet[rof]:\n",
    "            for word in set(news.split(' ')):\n",
    "                if not word in trainingCount[rof].keys():\n",
    "                    trainingCount[rof][word] = 1\n",
    "                else:\n",
    "                    trainingCount[rof][word] += 1\n",
    "        \n",
    "        for news in validationSet[rof]:\n",
    "            for word in set(news.split(' ')):\n",
    "                if not word in validationCount[rof].keys():\n",
    "                    validationCount[rof][word] = 1\n",
    "                else:\n",
    "                    validationCount[rof][word] += 1\n",
    "        \n",
    "        for news in testSet[rof]:\n",
    "            for word in set(news.split(' ')):\n",
    "                if not word in testCount[rof].keys():\n",
    "                    testCount[rof][word] = 1\n",
    "                else:\n",
    "                    testCount[rof][word] += 1\n",
    "\n",
    "    trainingRealCount = len(trainingSet['real'])\n",
    "    trainingFakeCount = len(trainingSet['fake'])\n",
    "    validationRealCount = len(validationSet['real'])\n",
    "    validationFakeCount = len(validationSet['fake'])\n",
    "    testRealCount = len(testSet['real'])\n",
    "    testFakeCount = len(testSet['fake'])\n",
    "    \n",
    "    return trainingRealCount,trainingFakeCount,validationRealCount,validationFakeCount,testRealCount,testFakeCount, trainingCount, validationCount, testCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(trainingRealCount,trainingFakeCount, trainingCount, m, pHat):\n",
    "    trainingProbability = {'real': {}, 'fake': {}}\n",
    "    for word in trainingCount['real'].keys():\n",
    "        trainingProbability['real'][word] = (trainingCount['real'][word]+m*pHat)/float(trainingRealCount+m)\n",
    "    for word in trainingCount['fake'].keys():\n",
    "        trainingProbability['fake'][word] = (trainingCount['fake'][word]+m*pHat)/float(trainingFakeCount+m)\n",
    "    \n",
    "    realProbability = float(trainingRealCount)/(trainingRealCount+trainingFakeCount)\n",
    "    fakeProbability = float(trainingFakeCount)/(trainingRealCount+trainingFakeCount)\n",
    "    \n",
    "    return trainingProbability, realProbability, fakeProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(trainingProbability,realProbability, fakeProbability, targetSet, m, pHat):\n",
    "    accuracy = 0\n",
    "    for rof in ['real', 'fake']:\n",
    "        for numNews in range(len(targetSet[rof])):\n",
    "            logProb = 0   \n",
    "            for word in trainingProbability['real'].keys():\n",
    "                if word in targetSet[rof][numNews].split(' '):\n",
    "                    logProb += log(trainingProbability['real'][word])\n",
    "                else:\n",
    "                    logProb += log(1-trainingProbability['real'][word])\n",
    "            for word in targetSet[rof][numNews].split(' '):\n",
    "                if not word in trainingProbability['real'].keys():\n",
    "                    logProb += log(pHat)\n",
    "            logProb += log(realProbability)\n",
    "            realProb = logProb\n",
    "\n",
    "            logProb = 0 \n",
    "            for word in trainingProbability['fake'].keys():\n",
    "                if word in targetSet[rof][numNews].split(' '):\n",
    "                    logProb += log(trainingProbability['fake'][word])\n",
    "                else:\n",
    "                    logProb += log(1-trainingProbability['fake'][word])\n",
    "            for word in targetSet[rof][numNews].split(' '):\n",
    "                if not word in trainingProbability['fake'].keys():\n",
    "                    logProb += log(pHat)\n",
    "            logProb += log(fakeProbability)\n",
    "            fakeProb = logProb\n",
    "            \n",
    "            if rof == 'real':\n",
    "                if realProb > fakeProb:\n",
    "                    accuracy += 1\n",
    "            else:\n",
    "                if realProb < fakeProb:\n",
    "                    accuracy += 1\n",
    "                    \n",
    "    return accuracy / (float(len(targetSet['fake']))+float(len(targetSet['real'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordBase(trainingSet):\n",
    "    wordBase = []\n",
    "    for rof in trainingSet.keys():\n",
    "        for news in trainingSet[rof]:\n",
    "            for word in news.split(' '):\n",
    "                wordBase.append(word)\n",
    "    return list(set(wordBase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbability(trainingProbability,realProbability, fakeProbability, wordBase, m, pHat):\n",
    "    words = []\n",
    "    realPs = []\n",
    "    fakePs = []\n",
    "    \n",
    "    for testWord in wordBase:\n",
    "        logProb = 0   \n",
    "        for word in trainingProbability['real'].keys():\n",
    "            if word == testWord:\n",
    "                logProb += log(trainingProbability['real'][word])\n",
    "            else:\n",
    "                logProb += log(1-trainingProbability['real'][word])\n",
    "        logProb += log(realProbability)\n",
    "        realProb = logProb\n",
    "        \n",
    "        logProb = 0   \n",
    "        for word in trainingProbability['fake'].keys():\n",
    "            if word == testWord:\n",
    "                logProb += log(trainingProbability['fake'][word])\n",
    "            else:\n",
    "                logProb += log(1-trainingProbability['fake'][word])\n",
    "        logProb += log(fakeProbability)\n",
    "        fakeProb = logProb\n",
    "        \n",
    "        words.append(testWord)\n",
    "        realPs.append(realProb)\n",
    "        fakePs.append(fakeProb)\n",
    "        \n",
    "    return words, realPs, fakePs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part2(seed = 0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "    validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "    validationCount, testCount = count(trainingSet, validationSet, testSet)\n",
    "    \n",
    "    m = 0.1\n",
    "    pHat = 0.0001\n",
    "    trainingProbability, realProbability, fakeProbability = \\\n",
    "    probability(trainingRealCount,trainingFakeCount, trainingCount, m,pHat)\n",
    "\n",
    "    print getAccuracy(trainingProbability,realProbability, fakeProbability, validationSet, m, pHat)\n",
    "    print getAccuracy(trainingProbability,realProbability, fakeProbability, testSet, m, pHat)\n",
    "    print getAccuracy(trainingProbability,realProbability, fakeProbability, trainingSet, m, pHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3Naive(seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    trainingProbability, realProbability, fakeProbability = probability(trainingRealCount,trainingFakeCount, trainingCount, m, pHat)\n",
    "    realWord = trainingProbability['real'].keys()\n",
    "    fakeWord = trainingProbability['fake'].keys()\n",
    "    realProbability = [trainingProbability['real'][prob] for prob in realWord]\n",
    "    fakeProbability = [trainingProbability['fake'][prob] for prob in fakeWord]\n",
    "    \n",
    "    realResult = []\n",
    "    for i in range(10):\n",
    "        realResult.append(realWord.pop(realProbability.index(max(realProbability))))\n",
    "        realProbability.remove(max(realProbability))\n",
    "    \n",
    "    fakeResult = []\n",
    "    for i in range(10):\n",
    "        fakeResult.append(fakeWord.pop(fakeProbability.index(max(fakeProbability))))\n",
    "        fakeProbability.remove(max(fakeProbability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3(seed = 0):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "    validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "    validationCount, testCount = count(trainingSet, validationSet, testSet)\n",
    "\n",
    "    m = 0.1\n",
    "    pHat = 0.0001\n",
    "    trainingProbability, realProbability, fakeProbability = \\\n",
    "    probability(trainingRealCount,trainingFakeCount, trainingCount, m,pHat)\n",
    "\n",
    "    wordBase = generateWordBase(trainingSet)\n",
    "    wordsOriginal, realPs, fakePs = getProbability(trainingProbability,realProbability, fakeProbability, wordBase, m, pHat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    \n",
    "    realResultMax = []\n",
    "    for i in range(10):\n",
    "        realResultMax.append(words.pop(realPs.index(max(realPs))))\n",
    "        realPs.remove(max(realPs))\n",
    "\n",
    "    words = deepcopy(wordsOriginal)\n",
    "    fakeResultMax = []\n",
    "    for i in range(10):\n",
    "        fakeResultMax.append(words.pop(fakePs.index(max(fakePs))))\n",
    "        fakePs.remove(max(fakePs))\n",
    "    \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    realResultMin = []\n",
    "    for i in range(10):\n",
    "        realResultMin.append(words.pop(realPs.index(min(realPs))))\n",
    "        realPs.remove(min(realPs))\n",
    "        \n",
    "    words = deepcopy(wordsOriginal)\n",
    "    fakeResultMin = []\n",
    "    for i in range(10):\n",
    "        fakeResultMin.append(words.pop(fakePs.index(min(fakePs))))\n",
    "        fakePs.remove(min(fakePs))\n",
    "        \n",
    "    return realResultMax, fakeResultMax, realResultMin, fakeResultMin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "realResultMax, fakeResultMax, realResultMin, fakeResultMin = part3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump',\n",
       " 'obstruction',\n",
       " 'controversial',\n",
       " 'aides',\n",
       " 'marching',\n",
       " 'stinks',\n",
       " 'paris',\n",
       " 'kidman',\n",
       " 'condemns',\n",
       " 'whack']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeResultMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump', 'to', 'the', 'donald', 'in', 'of', 'for', 'a', 'and', 'on']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(10):\n",
    "    result.append(fakeWord.pop(fakeProbability.index(max(fakeProbability))))\n",
    "    fakeProbability.remove(max(fakeProbability))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generateWordBase(trainingSet):  #defined above\n",
    "    wordBase = []\n",
    "    for rof in trainingSet.keys():\n",
    "        for news in trainingSet[rof]:\n",
    "            for word in news.split(' '):\n",
    "                wordBase.append(word)\n",
    "    return list(set(wordBase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(wordBase, targetSet):\n",
    "    xy = np.zeros([len(wordBase)+2, (len(targetSet['real'])+len(targetSet['fake']))])\n",
    "    n = 0\n",
    "    for rof in ['real', 'fake']:\n",
    "        for news in targetSet[rof]:\n",
    "            for word in news:\n",
    "                if word in wordBase:\n",
    "                    xy[wordBase.index(word),n] = 1\n",
    "            if rof == 'real':\n",
    "                xy[-1,n] = 1\n",
    "            n += 1\n",
    "    xy[-2,:] = 1\n",
    "    return xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sigmoidForward(y):\n",
    "    return 1/(1+exp(-y))\n",
    "\n",
    "def fcForward(x,W):\n",
    "    return np.matmul(W.T,x)\n",
    "    \n",
    "def forward(x, W):\n",
    "    L1 = fcForward(x,W)\n",
    "    output = sigmoidForward(L1)\n",
    "    return output\n",
    "\n",
    "def sigmoidBackward(y,output):\n",
    "    return output-y\n",
    "\n",
    "def fcBackward(dy,x):\n",
    "    dW = np.matmul(x,dy.T)\n",
    "    return dW\n",
    "\n",
    "def backward(y,output,x):\n",
    "    dy = sigmoidBackward(y,output)\n",
    "    dW = fcBackward(dy,x)\n",
    "    return dW\n",
    "\n",
    "def NLL(y, output):\n",
    "    return -sum(y*log(output)+(1-y)*log((1-output)))\n",
    "\n",
    "def backwardReg(y,output,x, W, lam):\n",
    "    dy = sigmoidBackward(y,output)\n",
    "    dW = fcBackward(dy,x)\n",
    "    dWR = 2*lam*W\n",
    "    return dW + dWR\n",
    "\n",
    "def NLLReg(y,output, lam, W):\n",
    "    return -sum(y*log(output)+(1-y)*log((1-output)))+ sum(lam*W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def grad_descent(f, df, loss, x, y, init_t,ada_learning_rate = True, alpha=0.0001,max_iter=1000, EPS = 1e-5):\n",
    "    \n",
    "    prev_t = init_t-10*EPS\n",
    "    t = init_t.copy()\n",
    "    iter  = 0\n",
    "\n",
    "    while norm(t - prev_t) >  EPS and iter < max_iter:\n",
    "        \n",
    "        #normal gradient descent\n",
    "        prev_t = t.copy()\n",
    "        t -= alpha*df(y, f(x,t), x)\n",
    "        \n",
    "        if iter % 5 == 0:\n",
    "            print \"Iter\", iter\n",
    "            currloss = loss(y, f(x,t))\n",
    "            print currloss\n",
    "        \n",
    "        iter += 1\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_regularization(f, df, loss, x, y, init_t,ada_learning_rate = True, \\\n",
    "                 tx=[], ty=[], figure_name = 'default',momentum = True, damping = 0.9, alpha=0.0001, \\\n",
    "                 max_iter=1000, EPS = 1e-5, zoom_in = 25):\n",
    "    v = np.zeros((init_t.shape[0],init_t.shape[1])) #used for momentum\n",
    "    lam = 0.0001 #######\n",
    "    prev_t = init_t-10*EPS\n",
    "    t = init_t.copy()\n",
    "    \n",
    "    currloss = loss(y, f(x,t), lam, t) # used for adaptive alpha\n",
    "    curr_alpha = alpha # used for adaptive alpha\n",
    "    \n",
    "    iter  = 0\n",
    "    num_iter = []\n",
    "    performance_training = []\n",
    "    performance_test = []\n",
    "    \n",
    "    while norm(t - prev_t) >  EPS and iter < max_iter:\n",
    "        \n",
    "        #normal gradient descent\n",
    "        prev_t = t.copy()\n",
    "        if momentum:\n",
    "            v = damping*v+curr_alpha*df(y, f(x,t), x, t, lam)\n",
    "            t -= v\n",
    "        else:\n",
    "            t -= curr_alpha*df(y, f(x,t), x, t, lam)\n",
    "        \n",
    "        if iter % 5 == 0:\n",
    "            print \"Iter\", iter\n",
    "            if currloss < loss(y, f(x,t), lam, t) and ada_learning_rate:\n",
    "                curr_alpha = curr_alpha/2\n",
    "            currloss = loss(y, f(x,t),lam, t)\n",
    "            print currloss\n",
    "        \n",
    "        iter += 1\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(o):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \"\"\"\n",
    "    y = 1/(1+np.exp(-o))\n",
    "    return y\n",
    "\n",
    "def loss_fn(y, t):\n",
    "    \"\"\"\n",
    "    loss function\n",
    "    \"\"\"\n",
    "    return sum(-t*np.log(y) - (1-t)*np.log(1-y))\n",
    "\n",
    "\n",
    "def f(x,t,W):\n",
    "    \"\"\"\n",
    "    combined function of forward part\n",
    "    \"\"\"\n",
    "    o = np.dot(x,W.T)\n",
    "    y = sigmoid(o)\n",
    "    return loss_fn(y, t)\n",
    "\n",
    "def df(x,t,W):\n",
    "    \"\"\"\n",
    "    combined function of back propagation part\n",
    "    \"\"\"\n",
    "    o = np.dot(x,W.T)\n",
    "    y = sigmoid(o)\n",
    "    lossGradient = y-t\n",
    "    combined_grad = np.dot(lossGradient.T, x)\n",
    "    return combined_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, df, loss, x, y, init_t,ada_learning_rate = True, alpha=0.0001,max_iter=1000, EPS = 1e-5):\n",
    "    \n",
    "    prev_t = init_t-10*EPS\n",
    "    t = init_t.copy()\n",
    "    iter  = 0\n",
    "\n",
    "    while norm(t - prev_t) >  EPS and iter < max_iter:\n",
    "        \n",
    "        #normal gradient descent\n",
    "        prev_t = t.copy()\n",
    "        t -= alpha*df(y, f(x,t), x)\n",
    "        \n",
    "        if iter % 5 == 0:\n",
    "            print \"Iter\", iter\n",
    "            currloss = loss(y, f(x,t))\n",
    "            print currloss\n",
    "        \n",
    "        iter += 1\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(x, y, W, print_output = True,report = \"test set\"):\n",
    "    output = forward(x,W)\n",
    "    result = 0.\n",
    "    print x.shape\n",
    "    for i in range(x.shape[1]):\n",
    "        if (output[0,i]>0.5 and y[i]==1):\n",
    "            result+=1\n",
    "        elif (output[0,i]<0.5 and y[i]==0):\n",
    "            result += 1\n",
    "            \n",
    "    if print_output:\n",
    "        print \"Performance on \"+report+ \": \" +str(result)+\"/\"+str(x.shape[1])+\"\\n\"\n",
    "    \n",
    "    return result/x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fakeNews, realNews = getNews()\n",
    "trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "wordBase = generateWordBase(trainingSet)\n",
    "\n",
    "training_xy = generateData(wordBase, trainingSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "1228.8643957063011\n",
      "Iter 5\n",
      "2013.106923007619\n",
      "Iter 10\n",
      "2215.1053398666145\n",
      "Iter 15\n",
      "1921.2931071288933\n",
      "Iter 20\n",
      "2048.7885317396767\n",
      "Iter 25\n",
      "1793.3365724795117\n",
      "Iter 30\n",
      "1886.2049073362864\n",
      "Iter 35\n",
      "1684.8504016208765\n",
      "Iter 40\n",
      "1754.2223109912147\n",
      "Iter 45\n",
      "1602.4953781112426\n",
      "Iter 50\n",
      "1661.2329619481407\n",
      "Iter 55\n",
      "1546.2112813004348\n",
      "Iter 60\n",
      "1601.4924825636106\n",
      "Iter 65\n",
      "1510.327191739977\n",
      "Iter 70\n",
      "1564.8045106131297\n",
      "Iter 75\n",
      "1488.0462497251092\n",
      "Iter 80\n",
      "1542.285393981389\n",
      "Iter 85\n",
      "1473.971003198493\n",
      "Iter 90\n",
      "1527.8822233674111\n",
      "Iter 95\n",
      "1464.581235748874\n",
      "Iter 100\n",
      "1517.9824042039795\n",
      "Iter 105\n",
      "1457.8274302185603\n",
      "Iter 110\n",
      "1510.5916596098816\n",
      "Iter 115\n",
      "1452.5890793327344\n",
      "Iter 120\n",
      "1504.6525643422128\n",
      "Iter 125\n",
      "1448.2689808956177\n",
      "Iter 130\n",
      "1499.610310318902\n",
      "Iter 135\n",
      "1444.547492505685\n",
      "Iter 140\n",
      "1495.1695145732817\n",
      "Iter 145\n",
      "1441.2484327932696\n",
      "Iter 150\n",
      "1491.1668785216725\n",
      "Iter 155\n",
      "1438.269588666164\n",
      "Iter 160\n",
      "1487.5066612608427\n",
      "Iter 165\n",
      "1435.547495376871\n",
      "Iter 170\n",
      "1484.128390266464\n",
      "Iter 175\n",
      "1433.0396645115513\n",
      "Iter 180\n",
      "1480.9906939748846\n",
      "Iter 185\n",
      "1430.7155389157313\n",
      "Iter 190\n",
      "1478.063115091713\n",
      "Iter 195\n",
      "1428.5517935365472\n",
      "Iter 200\n",
      "1475.3218715829478\n",
      "Iter 205\n",
      "1426.52981065724\n",
      "Iter 210\n",
      "1472.7475828892984\n",
      "Iter 215\n",
      "1424.6342571283526\n",
      "Iter 220\n",
      "1470.3239878679192\n",
      "Iter 225\n",
      "1422.8522325977779\n",
      "Iter 230\n",
      "1468.0371746001692\n",
      "Iter 235\n",
      "1421.1727236614602\n",
      "Iter 240\n",
      "1465.875083508663\n",
      "Iter 245\n",
      "1419.5862296758717\n",
      "Iter 250\n",
      "1463.8271634731573\n",
      "Iter 255\n",
      "1418.0844906737286\n",
      "Iter 260\n",
      "1461.8841189012448\n",
      "Iter 265\n",
      "1416.660280130428\n",
      "Iter 270\n",
      "1460.0377146894662\n",
      "Iter 275\n",
      "1415.3072417075045\n",
      "Iter 280\n",
      "1458.2806206361324\n",
      "Iter 285\n",
      "1414.0197575955478\n",
      "Iter 290\n",
      "1456.6062844132775\n",
      "Iter 295\n",
      "1412.7928406346814\n",
      "Iter 300\n",
      "1455.0088262254926\n",
      "Iter 305\n",
      "1411.6220449441687\n",
      "Iter 310\n",
      "1453.4829505211317\n",
      "Iter 315\n",
      "1410.5033913060138\n",
      "Iter 320\n",
      "1452.023871439032\n",
      "Iter 325\n",
      "1409.433304501013\n",
      "Iter 330\n",
      "1450.627249499936\n",
      "Iter 335\n",
      "1408.4085604358095\n",
      "Iter 340\n",
      "1449.2891376048692\n",
      "Iter 345\n",
      "1407.4262413526742\n",
      "Iter 350\n",
      "1448.0059347953745\n",
      "Iter 355\n",
      "1406.4836977499913\n",
      "Iter 360\n",
      "1446.7743465224353\n",
      "Iter 365\n",
      "1405.5785158991075\n",
      "Iter 370\n",
      "1445.5913503964819\n",
      "Iter 375\n",
      "1404.708490045111\n",
      "Iter 380\n",
      "1444.4541665691322\n",
      "Iter 385\n",
      "1403.8715985405152\n",
      "Iter 390\n",
      "1443.3602320408338\n",
      "Iter 395\n",
      "1403.065983290845\n",
      "Iter 400\n",
      "1442.3071783053229\n",
      "Iter 405\n",
      "1402.2899319967614\n",
      "Iter 410\n",
      "1441.2928118379414\n",
      "Iter 415\n",
      "1401.5418627640597\n",
      "Iter 420\n",
      "1440.3150970138126\n",
      "Iter 425\n",
      "1400.8203107235988\n",
      "Iter 430\n",
      "1439.3721411075205\n",
      "Iter 435\n",
      "1400.1239163620562\n",
      "Iter 440\n",
      "1438.4621810805882\n",
      "Iter 445\n",
      "1399.4514153124164\n",
      "Iter 450\n",
      "1437.5835719083223\n",
      "Iter 455\n",
      "1398.8016293935716\n",
      "Iter 460\n",
      "1436.734776235773\n",
      "Iter 465\n",
      "1398.173458721297\n",
      "Iter 470\n",
      "1435.9143551840762\n",
      "Iter 475\n",
      "1397.5658747408308\n",
      "Iter 480\n",
      "1435.1209601554522\n",
      "Iter 485\n",
      "1396.9779140543078\n",
      "Iter 490\n",
      "1434.3533255071538\n",
      "Iter 495\n",
      "1396.4086729354449\n"
     ]
    }
   ],
   "source": [
    "W = np.random.normal(0.,0.,[len(wordBase)+1,1])\n",
    "\n",
    "trainedW = grad_descent(f, df, NLL, training_xy[:-1,500:2500], training_xy[-1,500:2500], W,momentum = False, max_iter=500, alpha=0.0005,figure_name = \"part4f1\", tx=test_xy[:-1,:], ty=test_xy[-1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4791L, 1785L)\n",
      "Performance on training set: 1181.0/1785\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.661624649859944"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(training_xy[:-1,500:2500], training_xy[-1,500:2500], trainedW, print_output = True,report = \"training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part4(seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    wordBase = generateWordBase(trainingSet)\n",
    "    \n",
    "    training_xy = generateData(wordBase, trainingSet)\n",
    "    validation_xy = generateData(wordBase, validationSet)\n",
    "    test_xy = generateData(wordBase, testSet)\n",
    "    \n",
    "    W = np.random.normal(0.,0.,[len(wordBase)+1,1])\n",
    "    trainedW = grad_descent(forward, backward, NLL, training_xy[:-1,:], training_xy[-1,:], W,momentum = False, max_iter=2000, alpha=0.000,figure_name = \"part4f1\", tx=test_xy[:-1,:], ty=test_xy[-1,:])\n",
    "    \n",
    "    return training_xy, validation_xy, test_xy, trainedW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e2b47fa2cb0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_xy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_xy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_xy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainedW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpart4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-43e29703d809>\u001b[0m in \u001b[0;36mpart4\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwordBase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateWordBase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtraining_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtest_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-32dee0cdead8>\u001b[0m in \u001b[0;36mgenerateData\u001b[1;34m(wordBase, targetSet)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordBase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     \u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrof\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'real'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_xy, validation_xy, test_xy, trainedW = part4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5833L, 3266L)\n",
      "Performance on training set: 2334.0/3266\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7146356399265156"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(training_xy[:-1,:], training_xy[-1,:], trainedW, print_output = True,report = \"training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part4R(seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    fakeNews, realNews = getNews()\n",
    "    trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "    wordBase = generateWordBase(trainingSet)\n",
    "    \n",
    "    training_xy = generateData(wordBase, trainingSet)\n",
    "    validation_xy = generateData(wordBase, validationSet)\n",
    "    test_xy = generateData(wordBase, testSet)\n",
    "    \n",
    "    W = np.random.normal(0.,0.,[len(wordBase)+1,1])\n",
    "    trainedW = grad_descent_regularization(forward, backwardReg, NLLReg, training_xy[:-1,:], training_xy[-1,:], W,momentum = False, max_iter=2000, alpha=0.002,figure_name = \"part4f1\", tx=test_xy[:-1,:], ty=test_xy[-1,:])\n",
    "    \n",
    "    return training_xy, validation_xy, test_xy, trainedW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "5498.930542057853\n",
      "Iter 5\n",
      "7276.3319723988625\n",
      "Iter 10\n",
      "3119.43739976914\n",
      "Iter 15\n",
      "2447.0727180082044\n",
      "Iter 20\n",
      "3305.596354627229\n",
      "Iter 25\n",
      "1370.7261286397\n",
      "Iter 30\n",
      "1367.203789451116\n",
      "Iter 35\n",
      "1364.316101765613\n",
      "Iter 40\n",
      "1361.8709182133466\n",
      "Iter 45\n",
      "1359.7881980600423\n",
      "Iter 50\n",
      "1358.0017840519292\n",
      "Iter 55\n",
      "1356.4573664064972\n",
      "Iter 60\n",
      "1355.1106292612901\n",
      "Iter 65\n",
      "1353.9255896539012\n",
      "Iter 70\n",
      "1352.8731445208089\n",
      "Iter 75\n",
      "1351.9298240736207\n",
      "Iter 80\n",
      "1351.0767390222804\n",
      "Iter 85\n",
      "1350.2987030120223\n",
      "Iter 90\n",
      "1349.5835089751452\n",
      "Iter 95\n",
      "1348.9213377452602\n",
      "Iter 100\n",
      "1348.3042783677326\n",
      "Iter 105\n",
      "1347.7259414269338\n",
      "Iter 110\n",
      "1347.181148956536\n",
      "Iter 115\n",
      "1346.6656868146092\n",
      "Iter 120\n",
      "1346.176107615317\n",
      "Iter 125\n",
      "1345.709574317856\n",
      "Iter 130\n",
      "1345.2637363387018\n",
      "Iter 135\n",
      "1344.8366315669043\n",
      "Iter 140\n",
      "1344.4266089359198\n",
      "Iter 145\n",
      "1344.0322672617426\n",
      "Iter 150\n",
      "1343.6524069228617\n",
      "Iter 155\n",
      "1343.285991660598\n",
      "Iter 160\n",
      "1342.9321183448242\n",
      "Iter 165\n",
      "1342.5899930036642\n",
      "Iter 170\n",
      "1342.2589117771024\n",
      "Iter 175\n",
      "1341.9382457409922\n",
      "Iter 180\n",
      "1341.6274287744918\n",
      "Iter 185\n",
      "1341.3259478224356\n",
      "Iter 190\n",
      "1341.0333350444967\n",
      "Iter 195\n",
      "1340.7491614531168\n",
      "Iter 200\n",
      "1340.4730317284548\n",
      "Iter 205\n",
      "1340.2045799661241\n",
      "Iter 210\n",
      "1339.9434661662829\n",
      "Iter 215\n",
      "1339.6893733138975\n",
      "Iter 220\n",
      "1339.4420049322293\n",
      "Iter 225\n",
      "1339.2010830167756\n",
      "Iter 230\n",
      "1338.9663462765577\n",
      "Iter 235\n",
      "1338.737548625044\n",
      "Iter 240\n",
      "1338.5144578750048\n",
      "Iter 245\n",
      "1338.2968546010238\n",
      "Iter 250\n",
      "1338.0845311407702\n",
      "Iter 255\n",
      "1337.87729071192\n",
      "Iter 260\n",
      "1337.6749466261815\n",
      "Iter 265\n",
      "1337.4773215854525\n",
      "Iter 270\n",
      "1337.2842470479848\n",
      "Iter 275\n",
      "1337.0955626546577\n",
      "Iter 280\n",
      "1336.9111157072557\n",
      "Iter 285\n",
      "1336.730760692051\n",
      "Iter 290\n",
      "1336.5543588431326\n",
      "Iter 295\n",
      "1336.381777740831\n",
      "Iter 300\n",
      "1336.212890941322\n",
      "Iter 305\n",
      "1336.0475776340795\n",
      "Iter 310\n",
      "1335.8857223243456\n",
      "Iter 315\n",
      "1335.7272145381692\n",
      "Iter 320\n",
      "1335.5719485478967\n",
      "Iter 325\n",
      "1335.4198231162723\n",
      "Iter 330\n",
      "1335.2707412575194\n",
      "Iter 335\n",
      "1335.1246100139842\n",
      "Iter 340\n",
      "1334.9813402470604\n",
      "Iter 345\n",
      "1334.8408464412694\n",
      "Iter 350\n",
      "1334.703046520476\n",
      "Iter 355\n",
      "1334.5678616753257\n",
      "Iter 360\n",
      "1334.435216201078\n",
      "Iter 365\n",
      "1334.3050373450815\n",
      "Iter 370\n",
      "1334.1772551632096\n",
      "Iter 375\n",
      "1334.0518023846362\n",
      "Iter 380\n",
      "1333.9286142843735\n",
      "Iter 385\n",
      "1333.8076285630539\n",
      "Iter 390\n",
      "1333.6887852334723\n",
      "Iter 395\n",
      "1333.572026513445\n",
      "Iter 400\n",
      "1333.457296724577\n",
      "Iter 405\n",
      "1333.3445421965569\n",
      "Iter 410\n",
      "1333.233711176634\n",
      "Iter 415\n",
      "1333.1247537439458\n",
      "Iter 420\n",
      "1333.0176217284043\n",
      "Iter 425\n",
      "1332.9122686338508\n",
      "Iter 430\n",
      "1332.8086495652294\n",
      "Iter 435\n",
      "1332.7067211595304\n",
      "Iter 440\n",
      "1332.6064415202807\n",
      "Iter 445\n",
      "1332.507770155372\n",
      "Iter 450\n",
      "1332.41066791803\n",
      "Iter 455\n",
      "1332.3150969507387\n",
      "Iter 460\n",
      "1332.2210206319583\n",
      "Iter 465\n",
      "1332.1284035254632\n",
      "Iter 470\n",
      "1332.0372113321644\n",
      "Iter 475\n",
      "1331.9474108442673\n",
      "Iter 480\n",
      "1331.8589699016375\n",
      "Iter 485\n",
      "1331.7718573502511\n",
      "Iter 490\n",
      "1331.6860430026184\n",
      "Iter 495\n",
      "1331.6014976000647\n",
      "Iter 500\n",
      "1331.5181927767758\n",
      "Iter 505\n",
      "1331.4361010255093\n",
      "Iter 510\n",
      "1331.355195664881\n",
      "Iter 515\n",
      "1331.2754508081457\n",
      "Iter 520\n",
      "1331.1968413333939\n",
      "Iter 525\n",
      "1331.1193428550846\n",
      "Iter 530\n",
      "1331.0429316968525\n",
      "Iter 535\n",
      "1330.9675848655186\n",
      "Iter 540\n",
      "1330.8932800262426\n",
      "Iter 545\n",
      "1330.8199954787622\n",
      "Iter 550\n",
      "1330.7477101346612\n",
      "Iter 555\n",
      "1330.6764034956152\n",
      "Iter 560\n",
      "1330.6060556325679\n",
      "Iter 565\n",
      "1330.5366471657896\n",
      "Iter 570\n",
      "1330.468159245775\n",
      "Iter 575\n",
      "1330.4005735349417\n",
      "Iter 580\n",
      "1330.3338721900866\n",
      "Iter 585\n",
      "1330.2680378455664\n",
      "Iter 590\n",
      "1330.203053597165\n",
      "Iter 595\n",
      "1330.1389029866216\n",
      "Iter 600\n",
      "1330.0755699867757\n",
      "Iter 605\n",
      "1330.0130389873143\n",
      "Iter 610\n",
      "1329.951294781084\n",
      "Iter 615\n",
      "1329.8903225509434\n",
      "Iter 620\n",
      "1329.830107857135\n",
      "Iter 625\n",
      "1329.770636625146\n",
      "Iter 630\n",
      "1329.7118951340406\n",
      "Iter 635\n",
      "1329.653870005244\n",
      "Iter 640\n",
      "1329.596548191753\n",
      "Iter 645\n",
      "1329.539916967755\n",
      "Iter 650\n",
      "1329.4839639186412\n",
      "Iter 655\n",
      "1329.4286769313926\n",
      "Iter 660\n",
      "1329.3740441853256\n",
      "Iter 665\n",
      "1329.3200541431784\n",
      "Iter 670\n",
      "1329.2666955425257\n",
      "Iter 675\n",
      "1329.2139573875063\n",
      "Iter 680\n",
      "1329.1618289408527\n",
      "Iter 685\n",
      "1329.1102997162054\n",
      "Iter 690\n",
      "1329.0593594707052\n",
      "Iter 695\n",
      "1329.0089981978479\n",
      "Iter 700\n",
      "1328.9592061205922\n",
      "Iter 705\n",
      "1328.9099736847095\n",
      "Iter 710\n",
      "1328.8612915523677\n",
      "Iter 715\n",
      "1328.813150595935\n",
      "Iter 720\n",
      "1328.7655418920008\n",
      "Iter 725\n",
      "1328.7184567155996\n",
      "Iter 730\n",
      "1328.6718865346338\n",
      "Iter 735\n",
      "1328.6258230044823\n",
      "Iter 740\n",
      "1328.5802579627955\n",
      "Iter 745\n",
      "1328.535183424461\n",
      "Iter 750\n",
      "1328.4905915767379\n",
      "Iter 755\n",
      "1328.446474774554\n",
      "Iter 760\n",
      "1328.4028255359542\n",
      "Iter 765\n",
      "1328.3596365377025\n",
      "Iter 770\n",
      "1328.3169006110227\n",
      "Iter 775\n",
      "1328.2746107374805\n",
      "Iter 780\n",
      "1328.2327600449958\n",
      "Iter 785\n",
      "1328.1913418039826\n",
      "Iter 790\n",
      "1328.1503494236147\n",
      "Iter 795\n",
      "1328.1097764482056\n",
      "Iter 800\n",
      "1328.0696165537038\n",
      "Iter 805\n",
      "1328.0298635443003\n",
      "Iter 810\n",
      "1327.990511349136\n",
      "Iter 815\n",
      "1327.9515540191182\n",
      "Iter 820\n",
      "1327.9129857238272\n",
      "Iter 825\n",
      "1327.8748007485262\n",
      "Iter 830\n",
      "1327.8369934912548\n",
      "Iter 835\n",
      "1327.7995584600144\n",
      "Iter 840\n",
      "1327.7624902700375\n",
      "Iter 845\n",
      "1327.725783641139\n",
      "Iter 850\n",
      "1327.6894333951482\n",
      "Iter 855\n",
      "1327.6534344534116\n",
      "Iter 860\n",
      "1327.617781834377\n",
      "Iter 865\n",
      "1327.5824706512412\n",
      "Iter 870\n",
      "1327.5474961096727\n",
      "Iter 875\n",
      "1327.5128535055958\n",
      "Iter 880\n",
      "1327.4785382230416\n",
      "Iter 885\n",
      "1327.4445457320592\n",
      "Iter 890\n",
      "1327.410871586689\n",
      "Iter 895\n",
      "1327.3775114229895\n",
      "Iter 900\n",
      "1327.3444609571254\n",
      "Iter 905\n",
      "1327.311715983506\n",
      "Iter 910\n",
      "1327.2792723729758\n",
      "Iter 915\n",
      "1327.2471260710581\n",
      "Iter 920\n",
      "1327.2152730962448\n",
      "Iter 925\n",
      "1327.1837095383362\n",
      "Iter 930\n",
      "1327.152431556823\n",
      "Iter 935\n",
      "1327.1214353793162\n",
      "Iter 940\n",
      "1327.0907173000176\n",
      "Iter 945\n",
      "1327.0602736782323\n",
      "Iter 950\n",
      "1327.0301009369225\n",
      "Iter 955\n",
      "1327.000195561298\n",
      "Iter 960\n",
      "1326.970554097446\n",
      "Iter 965\n",
      "1326.941173150998\n",
      "Iter 970\n",
      "1326.9120493858302\n",
      "Iter 975\n",
      "1326.8831795227986\n",
      "Iter 980\n",
      "1326.8545603385096\n",
      "Iter 985\n",
      "1326.8261886641192\n",
      "Iter 990\n",
      "1326.7980613841662\n",
      "Iter 995\n",
      "1326.7701754354348\n",
      "Iter 1000\n",
      "1326.7425278058454\n",
      "Iter 1005\n",
      "1326.715115533377\n",
      "Iter 1010\n",
      "1326.6879357050134\n",
      "Iter 1015\n",
      "1326.6609854557175\n",
      "Iter 1020\n",
      "1326.6342619674338\n",
      "Iter 1025\n",
      "1326.6077624681136\n",
      "Iter 1030\n",
      "1326.581484230764\n",
      "Iter 1035\n",
      "1326.555424572525\n",
      "Iter 1040\n",
      "1326.5295808537635\n",
      "Iter 1045\n",
      "1326.5039504771953\n",
      "Iter 1050\n",
      "1326.478530887025\n",
      "Iter 1055\n",
      "1326.453319568109\n",
      "Iter 1060\n",
      "1326.4283140451387\n",
      "Iter 1065\n",
      "1326.403511881843\n",
      "Iter 1070\n",
      "1326.37891068021\n",
      "Iter 1075\n",
      "1326.3545080797294\n",
      "Iter 1080\n",
      "1326.3303017566498\n",
      "Iter 1085\n",
      "1326.306289423258\n",
      "Iter 1090\n",
      "1326.2824688271712\n",
      "Iter 1095\n",
      "1326.2588377506495\n",
      "Iter 1100\n",
      "1326.2353940099222\n",
      "Iter 1105\n",
      "1326.2121354545313\n",
      "Iter 1110\n",
      "1326.1890599666913\n",
      "Iter 1115\n",
      "1326.1661654606603\n",
      "Iter 1120\n",
      "1326.1434498821307\n",
      "Iter 1125\n",
      "1326.1209112076294\n",
      "Iter 1130\n",
      "1326.098547443936\n",
      "Iter 1135\n",
      "1326.0763566275104\n",
      "Iter 1140\n",
      "1326.0543368239373\n",
      "Iter 1145\n",
      "1326.0324861273796\n",
      "Iter 1150\n",
      "1326.0108026600474\n",
      "Iter 1155\n",
      "1325.9892845716781\n",
      "Iter 1160\n",
      "1325.9679300390264\n",
      "Iter 1165\n",
      "1325.946737265368\n",
      "Iter 1170\n",
      "1325.9257044800138\n",
      "Iter 1175\n",
      "1325.9048299378344\n",
      "Iter 1180\n",
      "1325.884111918795\n",
      "Iter 1185\n",
      "1325.8635487275023\n",
      "Iter 1190\n",
      "1325.8431386927587\n",
      "Iter 1195\n",
      "1325.8228801671276\n",
      "Iter 1200\n",
      "1325.8027715265096\n",
      "Iter 1205\n",
      "1325.7828111697256\n",
      "Iter 1210\n",
      "1325.7629975181098\n",
      "Iter 1215\n",
      "1325.7433290151116\n",
      "Iter 1220\n",
      "1325.7238041259063\n",
      "Iter 1225\n",
      "1325.7044213370132\n",
      "Iter 1230\n",
      "1325.6851791559236\n",
      "Iter 1235\n",
      "1325.666076110734\n",
      "Iter 1240\n",
      "1325.64711074979\n",
      "Iter 1245\n",
      "1325.6282816413357\n",
      "Iter 1250\n",
      "1325.609587373171\n",
      "Iter 1255\n",
      "1325.5910265523164\n",
      "Iter 1260\n",
      "1325.5725978046848\n",
      "Iter 1265\n",
      "1325.5542997747586\n",
      "Iter 1270\n",
      "1325.5361311252768\n",
      "Iter 1275\n",
      "1325.5180905369248\n",
      "Iter 1280\n",
      "1325.500176708032\n",
      "Iter 1285\n",
      "1325.4823883542786\n",
      "Iter 1290\n",
      "1325.4647242084009\n",
      "Iter 1295\n",
      "1325.447183019912\n",
      "Iter 1300\n",
      "1325.429763554821\n",
      "Iter 1305\n",
      "1325.4124645953598\n",
      "Iter 1310\n",
      "1325.3952849397178\n",
      "Iter 1315\n",
      "1325.3782234017785\n",
      "Iter 1320\n",
      "1325.3612788108624\n",
      "Iter 1325\n",
      "1325.344450011478\n",
      "Iter 1330\n",
      "1325.3277358630717\n",
      "Iter 1335\n",
      "1325.3111352397887\n",
      "Iter 1340\n",
      "1325.2946470302347\n",
      "Iter 1345\n",
      "1325.2782701372444\n",
      "Iter 1350\n",
      "1325.2620034776526\n",
      "Iter 1355\n",
      "1325.2458459820712\n",
      "Iter 1360\n",
      "1325.229796594671\n",
      "Iter 1365\n",
      "1325.2138542729654\n",
      "Iter 1370\n",
      "1325.1980179876005\n",
      "Iter 1375\n",
      "1325.1822867221488\n",
      "Iter 1380\n",
      "1325.166659472905\n",
      "Iter 1385\n",
      "1325.1511352486887\n",
      "Iter 1390\n",
      "1325.1357130706483\n",
      "Iter 1395\n",
      "1325.1203919720701\n",
      "Iter 1400\n",
      "1325.10517099819\n",
      "Iter 1405\n",
      "1325.0900492060096\n",
      "Iter 1410\n",
      "1325.0750256641143\n",
      "Iter 1415\n",
      "1325.0600994524982\n",
      "Iter 1420\n",
      "1325.0452696623872\n",
      "Iter 1425\n",
      "1325.0305353960691\n",
      "Iter 1430\n",
      "1325.0158957667263\n",
      "Iter 1435\n",
      "1325.0013498982705\n",
      "Iter 1440\n",
      "1324.986896925181\n",
      "Iter 1445\n",
      "1324.972535992346\n",
      "Iter 1450\n",
      "1324.958266254906\n",
      "Iter 1455\n",
      "1324.944086878103\n",
      "Iter 1460\n",
      "1324.9299970371271\n",
      "Iter 1465\n",
      "1324.915995916971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1470\n",
      "1324.9020827122845\n",
      "Iter 1475\n",
      "1324.8882566272323\n",
      "Iter 1480\n",
      "1324.8745168753533\n",
      "Iter 1485\n",
      "1324.8608626794255\n",
      "Iter 1490\n",
      "1324.8472932713278\n",
      "Iter 1495\n",
      "1324.8338078919098\n",
      "Iter 1500\n",
      "1324.8204057908627\n",
      "Iter 1505\n",
      "1324.8070862265874\n",
      "Iter 1510\n",
      "1324.793848466073\n",
      "Iter 1515\n",
      "1324.7806917847709\n",
      "Iter 1520\n",
      "1324.767615466474\n",
      "Iter 1525\n",
      "1324.7546188031981\n",
      "Iter 1530\n",
      "1324.7417010950637\n",
      "Iter 1535\n",
      "1324.7288616501821\n",
      "Iter 1540\n",
      "1324.7160997845403\n",
      "Iter 1545\n",
      "1324.703414821893\n",
      "Iter 1550\n",
      "1324.6908060936491\n",
      "Iter 1555\n",
      "1324.6782729387676\n",
      "Iter 1560\n",
      "1324.6658147036505\n",
      "Iter 1565\n",
      "1324.653430742039\n",
      "Iter 1570\n",
      "1324.6411204149115\n",
      "Iter 1575\n",
      "1324.6288830903825\n",
      "Iter 1580\n",
      "1324.616718143606\n",
      "Iter 1585\n",
      "1324.6046249566755\n",
      "Iter 1590\n",
      "1324.5926029185305\n",
      "Iter 1595\n",
      "1324.5806514248618\n",
      "Iter 1600\n",
      "1324.5687698780202\n",
      "Iter 1605\n",
      "1324.5569576869234\n",
      "Iter 1610\n",
      "1324.54521426697\n",
      "Iter 1615\n",
      "1324.533539039948\n",
      "Iter 1620\n",
      "1324.5219314339513\n",
      "Iter 1625\n",
      "1324.5103908832932\n",
      "Iter 1630\n",
      "1324.4989168284228\n",
      "Iter 1635\n",
      "1324.4875087158434\n",
      "Iter 1640\n",
      "1324.4761659980309\n",
      "Iter 1645\n",
      "1324.4648881333542\n",
      "Iter 1650\n",
      "1324.4536745859969\n",
      "Iter 1655\n",
      "1324.4425248258813\n",
      "Iter 1660\n",
      "1324.4314383285898\n",
      "Iter 1665\n",
      "1324.4204145752935\n",
      "Iter 1670\n",
      "1324.4094530526756\n",
      "Iter 1675\n",
      "1324.3985532528616\n",
      "Iter 1680\n",
      "1324.3877146733466\n",
      "Iter 1685\n",
      "1324.3769368169253\n",
      "Iter 1690\n",
      "1324.3662191916237\n",
      "Iter 1695\n",
      "1324.3555613106312\n",
      "Iter 1700\n",
      "1324.3449626922325\n",
      "Iter 1705\n",
      "1324.334422859743\n",
      "Iter 1710\n",
      "1324.3239413414442\n",
      "Iter 1715\n",
      "1324.313517670519\n",
      "Iter 1720\n",
      "1324.3031513849892\n",
      "Iter 1725\n",
      "1324.2928420276537\n",
      "Iter 1730\n",
      "1324.2825891460282\n",
      "Iter 1735\n",
      "1324.2723922922842\n",
      "Iter 1740\n",
      "1324.2622510231904\n",
      "Iter 1745\n",
      "1324.2521649000553\n",
      "Iter 1750\n",
      "1324.2421334886683\n",
      "Iter 1755\n",
      "1324.2321563592448\n",
      "Iter 1760\n",
      "1324.2222330863704\n",
      "Iter 1765\n",
      "1324.2123632489452\n",
      "Iter 1770\n",
      "1324.2025464301316\n",
      "Iter 1775\n",
      "1324.1927822172988\n",
      "Iter 1780\n",
      "1324.1830702019731\n",
      "Iter 1785\n",
      "1324.1734099797848\n",
      "Iter 1790\n",
      "1324.1638011504178\n",
      "Iter 1795\n",
      "1324.1542433175603\n",
      "Iter 1800\n",
      "1324.1447360888535\n",
      "Iter 1805\n",
      "1324.1352790758458\n",
      "Iter 1810\n",
      "1324.125871893943\n",
      "Iter 1815\n",
      "1324.1165141623624\n",
      "Iter 1820\n",
      "1324.107205504085\n",
      "Iter 1825\n",
      "1324.097945545811\n",
      "Iter 1830\n",
      "1324.0887339179146\n",
      "Iter 1835\n",
      "1324.0795702543985\n",
      "Iter 1840\n",
      "1324.0704541928512\n",
      "Iter 1845\n",
      "1324.0613853744032\n",
      "Iter 1850\n",
      "1324.0523634436856\n",
      "Iter 1855\n",
      "1324.043388048786\n",
      "Iter 1860\n",
      "1324.0344588412088\n",
      "Iter 1865\n",
      "1324.025575475835\n",
      "Iter 1870\n",
      "1324.0167376108807\n",
      "Iter 1875\n",
      "1324.0079449078569\n",
      "Iter 1880\n",
      "1323.9991970315332\n",
      "Iter 1885\n",
      "1323.9904936498972\n",
      "Iter 1890\n",
      "1323.9818344341159\n",
      "Iter 1895\n",
      "1323.973219058501\n",
      "Iter 1900\n",
      "1323.9646472004697\n",
      "Iter 1905\n",
      "1323.9561185405084\n",
      "Iter 1910\n",
      "1323.9476327621398\n",
      "Iter 1915\n",
      "1323.9391895518827\n",
      "Iter 1920\n",
      "1323.9307885992216\n",
      "Iter 1925\n",
      "1323.9224295965694\n",
      "Iter 1930\n",
      "1323.9141122392346\n",
      "Iter 1935\n",
      "1323.9058362253893\n",
      "Iter 1940\n",
      "1323.8976012560329\n",
      "Iter 1945\n",
      "1323.8894070349622\n",
      "Iter 1950\n",
      "1323.8812532687393\n",
      "Iter 1955\n",
      "1323.873139666658\n",
      "Iter 1960\n",
      "1323.8650659407149\n",
      "Iter 1965\n",
      "1323.8570318055765\n",
      "Iter 1970\n",
      "1323.849036978551\n",
      "Iter 1975\n",
      "1323.8410811795568\n",
      "Iter 1980\n",
      "1323.8331641310933\n",
      "Iter 1985\n",
      "1323.825285558212\n",
      "Iter 1990\n",
      "1323.8174451884877\n",
      "Iter 1995\n",
      "1323.8096427519897\n"
     ]
    }
   ],
   "source": [
    "training_xy, validation_xy, test_xy, trainedW = part4R()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4790"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
