{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC411 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import urllib\n",
    "# Constants\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used in multiple parts\n",
    "# -----------------------------------------------------------------------------\n",
    "def getNews(seed = 0):\n",
    "    \n",
    "    fakeFile = open(\"clean_fake.txt\",'r')\n",
    "    fakeNews = fakeFile.read().splitlines()\n",
    "    fakeFile.close()\n",
    "    realFile = open(\"clean_real.txt\",'r')\n",
    "    realNews = realFile.read().splitlines()\n",
    "    realFile.close()\n",
    "    \n",
    "    np.random.shuffle(fakeNews)\n",
    "    np.random.shuffle(realNews)\n",
    "    \n",
    "    return fakeNews, realNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSetSplit(fakeNews, realNews,seed = 0):\n",
    "    \n",
    "    trainingSet ={'real': realNews[:int(0.7*len(realNews))], 'fake':fakeNews[:int(0.7*len(fakeNews))]} \n",
    "    validationSet = {'real':realNews[int(0.7*len(realNews)):int(0.85*len(realNews))], 'fake':fakeNews[int(0.7*len(fakeNews)):int(0.85*len(fakeNews))]}\n",
    "    testSet = {'real': realNews[int(0.85*len(realNews)):] , 'fake': fakeNews[int(0.85*len(fakeNews)):]}\n",
    "\n",
    "    return trainingSet, validationSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(trainingSet, validationSet, testSet):\n",
    "    trainingCount = {'real':{}, 'fake':{}}\n",
    "    validationCount = {'real':{}, 'fake':{}}\n",
    "    testCount = {'real':{}, 'fake':{}}\n",
    "    \n",
    "    for rof in ['real','fake']:\n",
    "        for news in trainingSet[rof]:\n",
    "            for word in set(news.split(' ')):\n",
    "                if not word in trainingCount[rof].keys():\n",
    "                    trainingCount[rof][word] = 1\n",
    "                else:\n",
    "                    trainingCount[rof][word] += 1\n",
    "        \n",
    "        for news in validationSet[rof]:\n",
    "            for word in set(news.split(' ')):\n",
    "                if not word in validationCount[rof].keys():\n",
    "                    validationCount[rof][word] = 1\n",
    "                else:\n",
    "                    validationCount[rof][word] += 1\n",
    "        \n",
    "        for news in testSet[rof]:\n",
    "            for word in set(news.split(' ')):\n",
    "                if not word in testCount[rof].keys():\n",
    "                    testCount[rof][word] = 1\n",
    "                else:\n",
    "                    testCount[rof][word] += 1\n",
    "\n",
    "    trainingRealCount = len(trainingSet['real'])\n",
    "    trainingFakeCount = len(trainingSet['fake'])\n",
    "    validationRealCount = len(validationSet['real'])\n",
    "    validationFakeCount = len(validationSet['fake'])\n",
    "    testRealCount = len(testSet['real'])\n",
    "    testFakeCount = len(testSet['fake'])\n",
    "    \n",
    "    return trainingRealCount,trainingFakeCount,validationRealCount,validationFakeCount,testRealCount,testFakeCount, trainingCount, validationCount, testCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(trainingRealCount,trainingFakeCount, trainingCount, mReal, pHatReal, mFake, pHatFake):\n",
    "    trainingProbability = {'real': {}, 'fake': {}}\n",
    "    for word in trainingCount['real'].keys():\n",
    "        trainingProbability['real'][word] = (trainingCount['real'][word]+mReal*pHatReal)/float(trainingRealCount+mReal)\n",
    "    for word in trainingCount['fake'].keys():\n",
    "        trainingProbability['fake'][word] = (trainingCount['fake'][word]+mFake*pHatFake)/float(trainingFakeCount+mFake)\n",
    "    \n",
    "    realProbability = float(trainingRealCount)/(trainingRealCount+trainingFakeCount)\n",
    "    fakeProbability = float(trainingFakeCount)/(trainingRealCount+trainingFakeCount)\n",
    "    \n",
    "    return trainingProbability, realProbability, fakeProbability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "fakeNews, realNews = getNews()\n",
    "trainingSet, validationSet, testSet = dataSetSplit(fakeNews, realNews)\n",
    "trainingRealCount,trainingFakeCount,validationRealCount, \\\n",
    "validationFakeCount,testRealCount,testFakeCount, trainingCount, \\\n",
    "validationCount, testCount = count(trainingSet, validationSet, testSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "mReal = 0.1\n",
    "pHatReal = 0.0001\n",
    "mFake = 0.1\n",
    "pHatFake = 0.0001\n",
    "\n",
    "trainingProbability, realProbability, fakeProbability = \\\n",
    "probability(trainingRealCount,trainingFakeCount, trainingCount, mReal, pHatReal, mFake, pHatFake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on real news:\n",
      "0.928813559322\n"
     ]
    }
   ],
   "source": [
    "realAccuracy = 0\n",
    "\n",
    "for numNews in range(len(validationSet['real'])):\n",
    "    logProb = 0   \n",
    "    for word in trainingProbability['real'].keys():\n",
    "        if word in validationSet['real'][numNews].split(' '):\n",
    "            logProb += log(trainingProbability['real'][word])\n",
    "        else:\n",
    "            logProb += log(1-trainingProbability['real'][word])\n",
    "\n",
    "    for word in validationSet['real'][numNews].split(' '):\n",
    "        if not word in trainingProbability['real'].keys():\n",
    "            logProb += log(pHatReal)\n",
    "\n",
    "    logProb += log(realProbability)\n",
    "\n",
    "    realProb = logProb\n",
    "\n",
    "    logProb = 0 \n",
    "    for word in trainingProbability['fake'].keys():\n",
    "        if word in validationSet['real'][numNews].split(' '):\n",
    "            logProb += log(trainingProbability['fake'][word])\n",
    "        else:\n",
    "            logProb += log(1-trainingProbability['fake'][word])\n",
    "\n",
    "    for word in validationSet['real'][numNews].split(' '):\n",
    "        if not word in trainingProbability['fake'].keys():\n",
    "            logProb += log(pHatFake)\n",
    "\n",
    "    logProb += log(fakeProbability)\n",
    "\n",
    "    fakeProb = logProb\n",
    "\n",
    "    if realProb > fakeProb:\n",
    "        realAccuracy += 1\n",
    "    \n",
    "print \"Accuracy on real news:\"\n",
    "print realAccuracy / float(len(validationSet['real']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on fake news:\n",
      "0.805128205128\n"
     ]
    }
   ],
   "source": [
    "fakeAccuracy = 0\n",
    "\n",
    "for numNews in range(len(validationSet['fake'])):\n",
    "    logProb = 0   \n",
    "    for word in trainingProbability['real'].keys():\n",
    "        if word in validationSet['fake'][numNews].split(' '):\n",
    "            logProb += log(trainingProbability['real'][word])\n",
    "        else:\n",
    "            logProb += log(1-trainingProbability['real'][word])\n",
    "\n",
    "    for word in validationSet['fake'][numNews].split(' '):\n",
    "        if not word in trainingProbability['real'].keys():\n",
    "            logProb += log(pHatReal)\n",
    "\n",
    "    logProb += log(realProbability)\n",
    "    realProb = logProb\n",
    "\n",
    "    logProb = 0 \n",
    "    for word in trainingProbability['fake'].keys():\n",
    "        if word in validationSet['fake'][numNews].split(' '):\n",
    "            logProb += log(trainingProbability['fake'][word])\n",
    "        else:\n",
    "            logProb += log(1-trainingProbability['fake'][word])\n",
    "\n",
    "    for word in validationSet['fake'][numNews].split(' '):\n",
    "        if not word in trainingProbability['fake'].keys():\n",
    "            logProb += log(pHatFake)\n",
    "\n",
    "    logProb += log(fakeProbability)\n",
    "    fakeProb = logProb\n",
    "\n",
    "    if realProb < fakeProb:\n",
    "        fakeAccuracy += 1\n",
    "    \n",
    "print \"Accuracy on fake news:\"\n",
    "print fakeAccuracy / float(len(validationSet['fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
