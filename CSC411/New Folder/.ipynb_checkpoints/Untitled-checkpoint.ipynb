{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import urllib\n",
    "from copy import deepcopy\n",
    "\n",
    "def grad_descent(f, df, x, t, init_W, alpha):\n",
    "    \"\"\"\n",
    "    the gradient descent function without momentum\n",
    "    \"\"\"\n",
    "    print 'lalala'\n",
    "    EPS = 1e-5 \n",
    "    prev_W = init_W-10*EPS\n",
    "    W = init_W.copy()\n",
    "    max_iter = 3000\n",
    "    iter  = 0\n",
    "    print 'lalala'\n",
    "    while norm(W - prev_W) >  EPS and iter < max_iter:\n",
    "        print 'lalala'\n",
    "        prev_W = W.copy()\n",
    "        W -= alpha*df(x, t, W)\n",
    "        if iter % 1 == 0:\n",
    "            print \"Iter\", iter\n",
    "        iter += 1\n",
    "    return W\n",
    "\n",
    "def sigmoid(o):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \"\"\"\n",
    "    y = 1/(1+np.exp(-o))\n",
    "    return y\n",
    "\n",
    "def loss_fn(y, t):\n",
    "    \"\"\"\n",
    "    loss function\n",
    "    \"\"\"\n",
    "    return sum(-t*np.log(y) - (1-t)*np.log(1-y))\n",
    "\n",
    "\n",
    "def f(x,t,W):\n",
    "    \"\"\"\n",
    "    combined function of forward part\n",
    "    \"\"\"\n",
    "    o = np.dot(x,W.T)\n",
    "    y = sigmoid(o)\n",
    "    return loss_fn(y, t)\n",
    "\n",
    "def df(x,t,W):\n",
    "    \"\"\"\n",
    "    combined function of back propagation part\n",
    "    \"\"\"\n",
    "    o = np.dot(x,W.T)\n",
    "    y = sigmoid(o)\n",
    "    lossGradient = y-t\n",
    "    combined_grad = np.dot(lossGradient.T, x)\n",
    "    return combined_grad\n",
    "\n",
    "def performance(trainedW,x,t):\n",
    "    \"\"\"\n",
    "    evaluate the performance of the learning\n",
    "    \"\"\"\n",
    "    o = np.dot(x,trainedW.T)\n",
    "    y = sigmoid(o)\n",
    "    corrNum = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i,0] > 0.5:\n",
    "            y[i,0] = 1\n",
    "        else:\n",
    "            y[i,0] = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i,0] == t[i,0]:\n",
    "            corrNum += 1\n",
    "    return corrNum / y.shape[0]\n",
    "\n",
    "training_xy = np.load('training.npy')\n",
    "X = training_xy[:-1,:].T\n",
    "t = training_xy[-1, :]\n",
    "\n",
    "print t.shape\n",
    "print X.shape\n",
    "t.reshape([3266,1])\n",
    "W = np.random.normal(0., 0., (1,5833))\n",
    "trainedW = grad_descent(f, df, X, t, W, 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
